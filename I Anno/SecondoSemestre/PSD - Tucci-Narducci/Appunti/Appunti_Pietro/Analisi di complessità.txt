La complessità computazionale indica il costo degli algoritmi in termini di risorse di calcolo utilizzate, in particolare il tempo e lo spazio di memoria.

Per la valutazione del tempo bisogna considerare diversi fattori:
- La macchina usata;
- La configurazione dei dati;
- La dimensione dei dati;
- Il caso peggiore di configurazione dei dati;
- La funzione della dimensione dell'input;
- Il comportamento asintotico.

In generale si ricorre ad una macchina astratta che presenta le seguenti caratteristiche:
- Costo unitario delle istruzione e delle condizioni atomiche;
- Il costo delle strutture di controllo si ottiene dalla somma dei costi delle istruzioni contenute e delle condizioni;
- Le chiamate a funzione hanno costo pari al costo di tutte le loro istruzioni e controlli;
- Il passaggio di parametri ha costo nullo;
- Istruzioni e condizioni con chiamate a funzioni hanno costo pari al costo delle funzioni invocate più 1.

Prendiamo come problema esempio la verifica della presenza di un intero all'interno di un vettore ordinato in modo crescente:
Una ricerca che verifica cella per cella non trae alcun vantaggio dal fatto che il vettore è ordinato;
mentre una ricerca lineare con visita finalizzata soltanto nel caso peggiore non trarrebbe vantaggio.
Un ulteriore miglioramento lo si ha con la ricerca binaria, che anche nel peggiore dei casi presenta un vantaggio sostanziale.

Il caso peggiore è quello che produce il costo massimo a parità di dimensione,
il caso medio invece tiene conto dell'equiprobabilità dell'input.

Usiamo come esempio per i calcoli la ricerca sequenziale:
int ricerca(int v[], int size, int k)
{ int i;
   for (i=0; i<size; i++)
	if (v[i] == k) return i;
   return -1;
}

Dati: v[n] = {1, 3, 9, 17, 34, 95,96, 101} e k=9, si ha:
Inizializzazioni (i=0)	1 +
Confronti (i<size)		3 + 
Confronti (v[i]==k)		3 + 
Instruzioni (i++)		2 + 
Istruzioni (return i)	1 =
						10

Cosa cambia se k=10 ?

Inizializzazioni (i=0)		1 +
Confronti (i<size)			n+1 +
Confronti (v[i]==k)			n +
Istruzioni (i++)			n +
Istruzioni (return -1)		1 =
							3n+3

La probabilità che k sia in posizione i (1<=i<=n) vale 1/n
Costo del caso in posizione i: 3i+1
Costo caso medio:

(1/n) per la sommatoria che va da i = 1 ad n di (3i + 1) che vale (3n + 5)/2

Il costo come funzione della dimensione dell'input fa riferimento al numero di input che vengono ricevuti:
nel caso di un vettore, ad esempio, si tratta del numero di elementi.
Siccome al crescere della taglia n dell'input aumenta il tempo richiesto, viene studiato il comportamento asintotico,
cioè come si comporta la funzione per input di grandi dimensioni.
Gli algoritmi possono così venire divisi in classi di complessità organizzabili in una scala, 
trascurando tutte le costanti additive e moltiplcative, oltre che i termini di ordine inferiore:
- 1			costanti
- log(n)	logaritmica
- n			lineare
- n * log(n)
- n^2		quadratica
- n^3		cubica 
- a^n 		esponenziale
- n^n		esponenziale

Esistono due notazioni per confrontare la complessità di due funzioni: O e Omega
f(n) è O di g(n), cioè f(n) appartiene a O(g(n)), se esistono due costani c ed n0 tali che
se n >= n0, allora f(n) <= c per g(n)
Al contrario, f(n) è Omega di g(n) se per n >= n0 si ha c per g(n) <= f(n)
La notazione O limita superiormente una funzione, indicando che l'algoritmo è migliore dell'altro,
mentre la notazione Omega lo limita inferiormente, indicando che l'algoritmo è peggiore.

In generale, se nel caso pessimo occorre analizzare gli n dati di input, allora Omega(n) è un limite inferiore
alla complessità del problema (esempio: ricerca di un elemento in un vettore)

Esempio con dei frammenti di codice:
alg è la sequenza di alg1 ed alg2; alg1 è O(g1(n)), alg2 è O(g2(n)) 
i = 0;
while(i < n) {		g1(n)
print();
i++;
}

for(...){		g2(n)
scanf();
}

alg sarà quindi O di max(g2(n), g2(n))

Nel caso fossero stati frammenti annidati alg sarebbe stato O di (g1(n) * g2(n)).
Nel caso di sottoprogrammi ripetuti alg applica ripetutamente una serie di istruzioni la cui complessità vale f(n) ed il numero di volte è g(n):
in generale alg è O della sommatoria che va da i a g(n) di fi(n);
nel caso fi(n) fossero tutte uguali basta fare g(n) * f(n).

Per valutare la complessità di un algoritmo ricorsivo ci si basa sulle relazioni di ricorrenza e sul lavoro di combinazione,
che considera la preparazione delle chiamate ricorsive e la combinazione dei risultati ottenuti.

Per relazioni di ricorrenza che prevedono lavoro costante il lavoro di combinazione è indipendente dalla dimensione dei dati e si ha:
Una relazione lineare di ordine h come la sequenza di Fibonacci (h = 2) ha soluzione esponenziale con n,
mentre una come il fattoriale (h = 1) è di ordine lineare con n.
Le relazioni che prevedono partizioni di dati possono rientrare in due ordini:
il generico elemento C(n) ha forma a * C(n/p) + b, dove a è il numero di chiamate ricorsive e p il numero di partizioni
da cui l'ordine vale log(n) se a = 1, oppure n elevato a log in base p di a se a > 1
Nel caso specifico della ricerca binaria abbiamo complessità logaritmica (1 chiamata e 2 partizioni)

Per relazioni di ricorrenza con lavoro lineare il lavoro di combinazione è proporzionale alla dimensione dei dati e si ha:
Relazioni lineari di ordine h con ah = 1 e aj = 0 per 1 <= j <= h - 1
C(n) = C(n - h) + b*n + d	per n>h
La soluzione è di ordine quadratico in n
Relazioni con partizione dei dati prevedono invece 3 casi:
se a < p allora la soluzione è lineare;
se a = p allora n * log(n);
se a > p allora n elevato a log in base p di a.

Al secondo caso appartiene il Mergesort:
C(1) = c;
C(n) = 2 * C(n/2) + M(n) + c;
La complessità dell'algoritmo è O(n).
